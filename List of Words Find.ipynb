{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/alisdghnia/Desktop/PDF Whitepapers'\n",
    "for pdf_file in path:\n",
    "    def getPageCount(pdf_file):\n",
    "        pdfFileObj = open(pdf_file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pages = pdfReader.numPages\n",
    "        return pages\n",
    "\n",
    "\n",
    "    def extractData(pdf_file):\n",
    "        pdfFileObj = open(pdf_file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pages = pdfReader.getNumPages()\n",
    "        for i in range(pages):\n",
    "            currentPage = pdfReader.getPage(i)\n",
    "            text = currentPage.extractText()\n",
    "            words = len(text)\n",
    "        return words\n",
    "    \n",
    "    def getwordCount(data):\n",
    "        count = data.split()\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchForWord(pdf_file, search):\n",
    "    pdfFileObj = open(pdf_file, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    pages = pdfReader.getNumPages()\n",
    "    allPages = []\n",
    "    for i in range(pages):\n",
    "        currentPage = pdfReader.getPage(i)\n",
    "        text = currentPage.extractText()\n",
    "        if re.findall(search, text):\n",
    "            words = len(re.findall(search, text))\n",
    "            allPages.append((words, i))\n",
    "        \n",
    "    count = len(allPages)\n",
    "    total = sum([tup[0] for tup in allPages])\n",
    "    \n",
    "    return (total, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = []\n",
    "for (dirname,dirs,files) in os.walk('.'):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.pdf'):\n",
    "            thefile = os.path.join(filename)\n",
    "            words = ['bitcoin', 'crypto', 'token']\n",
    "            for search in words:\n",
    "                findingWordsNnumPages = searchForWord(filename, search)\n",
    "                wordList.append((filename, search, findingWordsNnumPages))\n",
    "                \n",
    "df = pd.DataFrame(wordList)\n",
    "os.chdir('/Users/alisdghnia/Desktop/Data CSV Files')\n",
    "df.to_csv('List of Words counted in each paper.csv',\n",
    "            index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
